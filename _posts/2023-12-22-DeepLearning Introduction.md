---
layout: article
title: Document - testing
mathjax: " true"
---



[课程链接](https://zh.d2l.ai/chapter_introduction/index.html#id2)

# Intro
利用**数据集**,对数据集的标记,利用机器学习算法,这一算法由许多的参数来决定,利用数据集确定当下的最佳数据集,参数在某种度量下达到了最佳的性能
以人工神经网络解微分方程为例,求解关于神经网络中的参数(包括每个神经元的权重,输出权重)的最优化问题,采取某种最优化算法,得到的最优解就训练出了一个合适的模型,从而可以反复利用与不同的 case.并且我们基于有限的数据实现了对于无穷多的数据的识别判断.也就是模糊的输入达到几乎准确的输出.

>[!KetWords]
>- dataset
>- parameter
>- model->model family
>- learning algorithm

模型的参数是模型族内的各类模型相互区别的原因.然而这样的参数不是随机设置的,我们利用标记好的数据进行 train,从而使得模型按照我们希望的方式执行.
>[!Note] 基本算法
>1. 初始的随机化俺叔的模型,模型不具有智能
>2. 获取样本(一般是有标签的)
>3. 调整参数,使模型在样本中表现的良好
>4. 重复二三步骤,直到在某种度量下达到要求

## 核心组件
- data
- 模型
- 目标函数
- 优化算法
### data
数据由不同的样本组成,每一个样本具由多个维度的特征(标签)来区分.
传统的我们需要把数据维度统一,但是深度学习可以处理不同长度的数据.
此外,数据的质量是重要的.垃圾的数据会导致模型的失败.不均衡的数据集会导致模型应用范围再改.数据的不公正性也会导致模型的不公正
### model
深度学习与经典方法的区别主要在于：前者关注的功能强大的模型，这些模型由神经网络错综复杂的交织在一起，包含层层数据转换，因此被称为深度学习（deep learning
### objective function
学习的成果用目标函数去衡量.我们一般把度量用数字去呈现.常见的例如
MSE,或采用其二次近拟.
一般我们把用于训练的数据集叫做 training set.训练得到模型之后,我们在另一些数据集上对模型进行测试,这样的用于测试的数据集我们叫做 test set.
### Optimization Method
对于目标函数,我们进行最小化算法实现,这样的过程就是最优化算法.大体思路来看,所有的这样的算法都是基于梯度下降的思路.然后尽可能快的去计算步长和方向,尽可能在较少的步数中实现收敛.
## 机器学习问题举例
### 监督学习(SL
这样的学习的目的是我们对于任何的输入特征,万能都可以映射预测或判断出一个标签.监督学习之所以能发挥作用，是因为在训练参数时，我们为模型提供了一个数据集，其中每个样本都有真实的标签。 
**“估计给定输入特征的标签”的条件概率**
![[Pasted image 20231216212128.png]]
基于目前特征及对应的标签,对与未来的标签进行预测是所有监督学习的特征
- 回归问题:当标签是多少,而非有限在某几个问题的值中取的时候
- 分类问题:当标签是两个(是或不是)或者可数(甚至有限个的时候)
  - 标记问题:将一个样本进行标签化而不仅仅是分类.也就是多充的分类,并且不相互影响
- 序列学习:当输入输出的大小不固定的时候,也是就特征向量的长度不一定时候.但是这样的模型只适用于对于特征之间独立同分布,相互之间不相关的时候.但是我们希望是的输入的内容可以被记住从而影响学习.例如 GPT 的记忆性.

### 无监督学习
自发的,没有目的的学习.也就是不知道什么是对的,什么是错的情况下.例如
- 聚类:在不知道数据标签标注的情况下,我们对数据分类
- 主成分分析:对于一个特征的反应结果,我们是否可以找到主要的影响因素.
- 推断因果关系
### 与环境互动
前所述都为 offline learning.我们的数据是不再更新的.
但是实际上我们的行为对环境是相关的,将模型与环境互动就得到了下面的强化学习的框架
### 强化学习
![[Pasted image 20231216215616.png]]

# 2-Preludes 
## 2.1 数据操作
知识总结:
`tensor` 就类似于 `numpy` 中的`ndarray` ,`randn`类似的操作是一样的 比如 arange生成,shape ,reshape,shape size,numel.
基本的运算符号是 elementwise 的例如加减乘除平方,包括 exp 等.对于== 等也是pointwise 的返回结果.
广播机制是类似的,就是通过复制使得可以运算.
### NEW
- 张量
可以看作是层数的叠加,`X.shape==(2,3,4)`的实际形状是
```python
tensor([[[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]])
```
- 为了节省内存
原来写作
```python
X = X+Y
```
的操作可以写成
```
Y[:] = Y+X
```
### Homework

### 广播
对 `a+b` 对 a 复制列,对 b 复制行
[summary for 广播](https://blog.csdn.net/flyingluohaipeng/article/details/125107959)
- 广播的机制:在 shape 不一样的时候,取两个张量的各个维度的最大值进行复制
A.shape=(1,9,4),B.shape=(15,1,4),那么A+B的shape是(15,9,4)
- 广播的条件:
2.1 完结
### 张量
要理解广播,先得理解 tensor.对于一个 shape 为(l,m,n)的 tensor
我们把每个数字看做最小的单位,是标量,那比标量大一级就构成了一维的 tensor 就是由 n 决定的.n 代表了一个一维度的 tensor 中具有的元素的个数.(也就是可以把一维的 tensor 看作是一个数字集合)
*为啥从 n 个元素构成的序列开始考虑维数*:对于一个数字而言,个数永远是 1,是平凡的.
由很多个都是 n 由 n 个数字构成的一维 tensor 构成了一个 tensor 族(可以看作是集合类).对于这些 tensor 族我们规定了维度 m,也就是每 m 个集合(一维 tensor)构成一个 tensor 族.
剩下的 l 参数在数据固定的时候实际上就是在 m,n 固定的情况下固定的了
## 2.2 数据预处理
- 创建/读取数据集
#unsolved os
### fill_na
There is a example of fill the NaN with intercept
```python
inputs,outputs = data.iloc[:,0:2], data.iloc[:,2]
inputs = inputs.fillna(inputs.mean())
```
For the array that is discrete value, we have the process like this
```python
inputs  = pd.get_dummies(inputs,dummy_na=True)
```
### 2.2.3 张量格式
```python
import torch
X = torch.tensor(inputs.to_numpy(dtype=float))
Y = torch.tensor(outputs.to_numpy(dtype = float))
```
## 2.3 Linear algebra with torch
- 标量
```python
import torch
x = torch.tensor(3.)
y = torch.tensor(2.)
x+y,x*y...
```
- vector
```python
#create
x = torch.arange(9)
#find
x1 = x[0]
```
- 形状
  ```python
x = torch.arange(12).reshape(3,4)
x.shape#torch.Size([3,4])
m,n = x.shape#m =3,n=4
```
- 矩阵
```python
A = torch.arange(20).reshape(5,4)
A.T
```

```python
B = torch.tensor([[1,2,3],[2,0,4],[3,4,5]])
```
### 张量
对于传统的运算符号`+ - * /` 当两个张量的形状相同的时候,执行的是对应元素之间相互操作,那么返回的张量也是和原来的两个张量形状一致的张量.

Hadamard 积:对应位置的元素与对应位置的元素相互乘积

张量和一个标量之间的张量是通过标量对每一个张量中的元素操作实现的

### 降维的方式
求和
```python
x = torch.arange(3,dtype = torch.float32)
x.sum()
```
此时的 sum 操作可以对所有维度的张量进行.
- default 的情况下,操作是返回一个标量的,也就是对所有的元素求和
 ```
x = torch.arange(12).reshape(3,4)
tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])

x.sum()
tensor(66)
```

  但是我们也可以指定维度使得它只降低部分维度(比如把日收入化为月收入) 这时候的操作是加入参数 `axis=`
```
x.sum(axis=0)
tensor([12, 15, 18, 21])
```

- axis=0 是纵向的/axis=1 是指列轴
而不指定参数时候,事实上就是进行两次求和 等价于操作
类似的还有如下的操作
`A.mean(axis)` $\iff$ `A.sum(axis)/A.shape[]`

具体轴与数据的对应如下

```
x
tensor([[[ 0,  1,  2,  3,  4],
         [ 5,  6,  7,  8,  9]],
        [[10, 11, 12, 13, 14],
         [15, 16, 17, 18, 19]]])
x.sum(axis = 0)
tensor([[10, 12, 14, 16, 18],
        [20, 22, 24, 26, 28]])
x.sum(axis = 1)
tensor([[ 5,  7,  9, 11, 13],
        [25, 27, 29, 31, 33]])
x.sum(axis = 2)
tensor([[10, 35],
        [60, 85]])

```

```
x
tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])

x.sum(axis = 0)
tensor([12, 15, 18, 21])

x.sum(axis = 1)
tensor([ 6, 22, 38])

```

#### 非降维求和
非降维求和只需要添加参数 keepdims
`x.sum(axis = 1,keepdims = True)`

这样可以广播,否则一维与二维之间的无法广播

### 点积
- 向量与向量
一维张量之间我们使用
`torch.dot(a,b)`
- 矩阵与向量
对于矩阵与向量之间,我们采用下面的模式
`torch.mv(A,x)`
而且要求 A 的列数(axis = 1) 与向量的维数一致
- 矩阵与矩阵
`torch.mm(A,B)`
要求 A 的 axis = 1 与 B 的 axis = 0 一致

>[!Note]
>张量的维度shape `(m,n,k)` 是按照从高维到低维排列的, `m`是二维矩阵的个数, `n` 是每个矩阵中的行数 `k` 是每个行中的元素个数.这就是解释了之前的 sum 对于张量的 case
>- 按照axis = 0 求和的时候,是两个矩阵中对应位置的元素相加
>- 按照 axis = 1 求和的时候是一个矩阵中的上下两列对应位置求和
>- 按照 axis = 2 求和的时候是每一行中的元素求和
>  更加直观的, axis = i 就是站在何种维度上,把元素相互求和



### 范数
- 一般的`troch.norm(u)` 计算的都是二范数
范数的性质是显然的

