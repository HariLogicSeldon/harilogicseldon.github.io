---
layout: article
title: Linear Neural Networks for Regression-D2L-3.all
tags: 
"last_modified_at:": 2024-02-08 16:03
share: "true"
---

# Linear Regression
## Generalization
对于机器学习,我们希望他具有良好的泛化能力,我们希望挖掘更深层次的模式,而不是表征.在现实生活中,我们通常使用有限的数据集去训练模型.一般说来,会遇到两类问题
- 欠拟合:训练误差过大
- 过拟合:训练误差远小于泛化误差
## 训练误差与泛化误差
在一个一般的监督学习中,我们总是假设训练数据/测试数据直接是独立同分布的.然而这样的假设显然是过于严格了.但是另一方面,脱离了这样的假设,我们的理论无法得到应用.另一个问题是,为什么我们基于一种分布假设下的训练适用于对于另外一种分布的预测呢?
我们先解决第一个问题.首先我们需要区分训练误差与泛化误差,欠着是在训练数据集上的统计量,后者则是对于分布的期望.你可以把泛化误差当做是如果把模型应用于无限的相同分布的数据集中的结果.在误差上,训练误差就是至多可列和,但是泛化误差是一个积分.
但是事实上,我们并没有办法计算泛化误差.因为我们并不知道分布,但是我们现在的问题就是求解分布.并且我们也不可能得到一个无限数据集.在生产实践中,我们实际上是通过将我们的模型应用于一个独立的测试集中来==估计== 泛化误差.
在对模型进行评估的时候,我们实际上的误差估计只是一个简单的均值估计.但是对于训练集来说,并不是这样.我们的模型参数依赖于训练集的选择.这会导致此时的训练误差实际上是对分布的真是误差的有偏估计.泛化的核心问题是我们什么时候应该希望我们的训练误差接近于我们的总体误差.
## 模型复杂性
在传统的理论中,我们都是简单模型和充足的数据.训练误差与泛化误差一般是很接近的.然而,当我们处理复杂模型和少量的数据的时候,我们可以预见训练误差较小但是泛化误差可以增大.这并不惊奇.考虑这样的一个模型,他对于任何的 $n$ 大小的数据集都可以找到完美符合 label 的参数.这样一来,即使我们对于我们的训练数据拟合的很好,我们也很难知道这其中的泛化误差.一般看来,这样的泛化误差并不会比瞎猜好多少.
一般来说在不基于任何的限制下,我们并不能仅仅基于拟合训练数据来寻找泛化模式.另一方面,如果我们的模型并无法对任何的标签都拟合,这说明他肯定发现了一种模式.基于 Popper 的理论,一个可以解释任何和所有观察的理论根本就不是一个科学理论.毕竟这并没有排除任何的可能性.
以多项式拟合为例,给定一个由标量数据特征 $x$ 和对应的标量标签 $y$ 组成的训练集.多项式你和的目标是寻找一个多项式去近似 $y$
随着多项式阶数的增长,我们的参数也会变多,模型函数的选择空间也就不断的扩大.这会导致高阶多项式容易获得更大的训练误差.但是另一方面,这也会导致泛化误差会很在某个值之后也开始增大.这就导致了过拟合的问题.
### 训练数据集
训练数据集的大小也是造成这一问题的重要原因.若训练数据集中样本数很小,特别是比模型参数数量小的多的时候,过拟合容易产生.此外,泛化误差并不会随着样本数量增加而改变
## 模型选择
在机器学习中,对于不同的实际问题,实际上会对应着不同的适合的模型,所以我们实际上需要面对如何挑选合适的模型的问题.下面是一般化的挑选模型的流程
### 验证数据集
严格来说,测试集只能在所有超参数和模型参数固定后使用一次,不可以用测试数据调整参数.由于无法从训练误差中来估计泛化误差,因此也不应该只依赖于训练数据选择模型.从而我们常常会从数据集中挑选出一部分留作验证集.
### k fold cross validation
由于训练数据集不参与模型训练,当训练数据集不够用的时候,预留过多数据验证是 expensive的选择,一种方式是使用 kFCV. 我们把原始的训练数据集分割为 k 个不重合的子数据集,然后我们进行 k 次模型训练和验证,每一次我们使用一个子数据集作为验证集,而其余的集合是测试集合.最终我们用 k 次的训练误差和验证误差分别求平均

## Weight decay
除了增大数据集,我们还可以使用 WD 的方法解决过拟合的问题.
### 方法
权重衰减等价于 $L_{2}$ 范数正则化.正则化为模型损失函数添加惩罚项,从而使得模型的权重参数较小,是一种解决过拟合的手段.
$L_{2}$ 范数正则化,在模型的原损失函数上加入了 $L_{2}$ 范数惩罚项.他是指模型权重参数的每个元素的平方和与一个正常数的乘积若我们假设 LossFunction 是 MSE,表示为
$$
\mathscr{l}(w_{1},w_{2},b) = \frac{1}{n} \sum_{i=1}^n \frac{1}{2} (x_{1}^{(i)}w_{1}+x_{2}^{(i)}w_{2}+b-y^{(i)})^2
$$
我们改造他为
$$
\mathscr{l}(w_{1},w_{2},b) +\frac{\lambda}{2}\lvert \mathbf{}{w} \rvert ^2
$$
其中的 $\lambda$ 是超参数.大于零.这就使得 $w$ 本身会维持在一个较小的值.这样的方式使得权重变小了,所以叫做权重衰减.
