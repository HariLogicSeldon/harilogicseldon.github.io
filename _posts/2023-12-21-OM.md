---
title: 负梯度/Newton方法-OptimizationMethod-030
---

- 本章的重点在于对方向的寻找,步长的寻找都假设已知了,那么我们也就一般写作 $\alpha$从而使得我们更加关注与对方向的寻找.
# 知识补充
[潘老师数值分析讲义](https://math.ecnu.edu.cn/~jypan/Teaching/NA/main_na_202308.pdf)  [[数值分析(施工中)]]
[完整项目代码](https://github.com/HariLogicSeldon/2023_Fall_Optimization)

-  矩阵范数
![[Pasted image 20231019081934.png]]

# Sect 3.1 最速下降法
## 方法概述
考虑目标函数 $f(x)$的一阶Taylor 展开有
$$
f(x+\alpha d_k)=f(x)+\alpha g_k^Td_k+O(||d||^2)
$$
那么此时,作为"最速下降法",我们的想法就是在步长取定的时候一次步骤中,确定下降最多的的方向,又由于 $g_k^Td_k<0$ 那么,也就是求
$$
\min_{d_k} g_kd_k
$$
这时候,利用 Cauchy-Schwarz 不等式,我们知道
$$
 |g_k^Td_k|\leq ||g_k||\cdot||d_k||
$$
等号成立当且仅当 
$$
d_k=-g_k
$$
这时候的方法就是 **最速下降法(SD)**.
#algorithm
> [!math|{"type":"conjecture","number":"auto","setAsNoteMathLink":true,"title":"Steepest Decent","label":"steepest-decent","_index":0}] Algorithm 1 (Steepest Decent).
> 1. 给定初值 $x_0\in \mathbb{R},\varepsilon>0,k:=0$
> 2. 终止条件
> 3. $d_k=-g_k$
> 4. 一维精确线搜索求步长
> 5. $x_{k+1}:=x_k+\alpha_k d_k,k:=k+1,$ 转步骤 2

- 对于二次函数,我们知道了若他的二次型矩阵是 $B$,那么
$$
  \alpha_k=-\frac{g_kg_k^T}{g_kBg_k^T}
$$
但是对于一般的函数,就只能求解问题
$$
\min_{\alpha}f(x_k+\alpha_kd)\iff g^T(x_k+\alpha_k d)d=0
$$

从而获得步长.

## 最速下降法收敛特点
### 收敛性
由方向的选取方式知道 $\theta_k=0$,那么成立定理2.8,从而具有全局收敛性
### 收敛速度
由于一般的函数可以用正定二次函数逼近,那么我们只考虑 **正定二次函数**最速下降发的收敛速度.一般的结果待仔细研读


正定二次函数的步长,我们已经在前面求得,从而知道了迭代点的情况
$$
x_{k+1}=x_k-\frac{g_k^Tg_k}{g_k^TGg_k}g_k
$$
总之,我们会得到
> [!math|{"type":"theorem","number":"auto","setAsNoteMathLink":false,"_index":1}] Theorem 2.
> 对于正定二次函数,其二次型矩阵为 $G$ 的时候,我们知道他的 SD 方法收敛速度满足
>$$
>\frac{||x_{k+1}-x^*||^2_G}{||x_{k}-x^*||^2_G}\leq (\frac{\lambda_{max}-\lambda_{min}}{\lambda_{max}+\lambda_{min}})^2
>$$

### cond(G)对问题的影响
由于此时考虑正定二次函数,那么我们不放去考察哦 cond(G),注意到收敛速度的左边可以写作
$$
\mu=\frac{cond(G)-1}{cond(G)+1}
$$
那么我们知道,cond(G)越接近于 1 的时候 ,$\mu$ 越小,收敛速度越接近于超线性收敛.cond(G)越大, $\mu$ 越大,收敛速度越慢.
同时我们也知道条件数过大的时候,条件数就是误差的放大倍数,条件数过大会导致误差扰动很大,此时问题是病态的,因此希望条件数尽量小(最小就是 1.)

|收敛性|收敛速度|存储量|计算量|
|---|---|--|--|
|全局收敛(对初始点依赖性低)|正定二次函数收敛速度线性|小|小|
下面是算法代码部分

``` python
import matplotlib.pyplot as plt
import numpy as np

np.set_printoptions (suppress= True)
x_1=np.linspace(-110,110,300)
x_2=np.linspace(-110,110,300)
G = np.array([[21,4],
                  [4,15]
                  ])
b = np.array([2,3]).T
c = 10
def f(x_1,x_2):
    return (G[0,0]*x_1**2+G[1,1]*x_2**2)/2+(G[0,1]+G[1,0])*x_1*x_2+b[0]*x_1+b[1]*x_2+10

Y,X = np.meshgrid(x_1,x_2)
z = f(X,Y)

x = np.array([[-30,100]])
def gk(x):
    return np.array([[G[0,0]*x[0]+G[1,0]*x[1]+2,G[0,1]*x[0]+G[1,1]*x[1]]])
def alpha(gk):
    return (gk@gk.T)/(gk@G@gk.T)
k=0
g=[np.linalg.norm(gk(x[0]))]
while g[k]>=1e-5:
    d = -gk(x[k])
    alpha0 = alpha(d)
    xk1 = x[k] + alpha0[0][0] * d
    x = np.r_[x, xk1]
    g.append(np.linalg.norm(d))
    k = k + 1

x = np.around(x,decimals=4)
print(len(x))
print(x)
print(len(g))
print(g)
x = x.T
# fig= plt.figure(figsize=(10,10))
# ax = plt.subplot(121)
# ax.plot_surface(X, Y, z, vmin=z.min() * 2, cmap="viridis")
#
# ax.set(xticklabels=[],
#        yticklabels=[],
#        zticklabels=[])
# ax2 = plt.subplot(122)
plt.contour(X,Y,z,20)

plt.scatter(x[0],x[1],c='r')
plt.plot(x[0],x[1],c='r')
plt.show()
```
-----
# Sect 3.2 Newton 方法
>[!核心思想]
> 在迭代点附近用二次函数拟合原函数(丢掉泰勒展开中的 $o(||d||^2)$)

在这一过程中我们利用的目标函数是
$$
\min q_k(d)=f_k+g_k^T+\frac{1}{2}d^T G_k d
$$
极值点满足**牛顿方程**
$$
G_kd=-g_k
$$
倘若收敛,*我们规定 $G_k$需要正定*,那么就得到了牛顿方向
$$
d=-G_k^{-1}g_k
$$
所以对于所有的牛顿方法我们的基本要求就是
- ==$G_k$正定==
从而也会具有一个公共的性质
- 由牛顿方程决定的方向都是下降方向
- 并且牛顿方法都是二阶收敛的
## 基本牛顿法

在我们简化步长的想法的情况下,我们有基本牛顿法,也就是取步长为 1 恒定的时候.
- 基本 Newton 方法   
```python
import numpy as np
import matplotlib.pyplot as plt
np.set_printoptions (suppress= True)
def f(x_1,x_2):
    return 3*x_1**2+3*x_2**2-(x_1**2)*x_2

x1=np.linspace(-6,6,100)
x2=np.linspace(-6,6,100)
X,Y = np.meshgrid(x1,x2)
Z = f(X,Y)
#Newton
def g(x1,x2):
    return np.array([6*x1-2*x1*x2,6*x2-x1**2])
def G(x1,x2):
    return np.array([
        [6-2*x2,-2*x1],
        [-2*x1,6]])
x = np.array([[1.5,1.5]])
gk = []

for k in range(1000):
    GG = G(x[k][0],x[k][1])
    gg = g(x[k][0],x[k][1])
    if k<999:
        if np.linalg.norm(gg)>=1e-6:
            if np.linalg.det(GG)==0:
                print("G is not suitable,Algorithm Failed")
                break
            else:
                d = -(np.linalg.inv(GG) @ gg.T)
                xk = x[k] + d
                x = np.r_[x, [xk]]
                gk.append(gg)
        else:
            print("succeed")
            break
    else:
        print("Try too many times")
print(x)

xx = x.T[0]
yy = x.T[1]

#plot
fig, ax2 = plt.subplots()
levels = np.linspace(np.min(Z), np.max(Z), 20)
ax2.contour(X, Y, Z,levels=levels)
ax2.scatter(xx,yy,c='r')
ax2.plot(xx,yy)
plt.show()
```
- 基于二次函数改造的基本 Newton 方法
```python
import numpy as np
import matplotlib.pyplot as plt
np.set_printoptions (suppress= True)

G = np.array([[21,4],
                  [4,1]
                  ])
b = np.array([2,3]).T
c = 10
def f(x_1,x_2):
    return (G[0,0]*x_1**2+G[1,1]*x_2**2+(G[0,1]+G[1,0])*x_1*x_2)/2+b[0]*x_1+b[1]*x_2+10

x1=np.linspace(-15,15,100)
x2=np.linspace(-15,15,100)
X,Y = np.meshgrid(x1,x2)
Z = f(X,Y)
#Newton
def g(x1,x2):
    return np.array([G[0,0]*x1+G[0,1]*x2+b[0],G[1,0]*x1+G[1,1]*x2+b[1]])
def Gk(x1,x2):
    return G
x = np.array([[0,0]])
gk = []

for k in range(1000):
    GG = Gk(x[k][0],x[k][1])
    gg = g(x[k][0],x[k][1])
    if k<999:
        if np.linalg.norm(gg)>=1e-6:
            if np.linalg.det(GG)==0:
                print("Algorithm Failed")
                break
            else:
                d = -(np.linalg.inv(GG) @ gg.T)
                xk = x[k] + d
                x = np.r_[x, [xk]]
                gk.append(gg)
        else:
            print("succeed")
            break

    else:
        print("Algorithm Failed")
print(x)

xx = x.T[0]
yy = x.T[1]

#plot
fig, ax2 = plt.subplots()
levels = np.linspace(np.min(Z), np.max(Z), 20)
ax2.contour(X, Y, Z,levels=levels)
ax2.scatter(xx,yy,c='r')
ax2.plot(xx,yy)
plt.show()
```
值得注意的是,该方法一步就一定收敛,甚至**不用考虑初值选取的良好性**,原因从基本牛顿法的选取中可以直接看出.事实上更基础的,我们在考虑二次函数时候总是可以直接求得他的极小值/极大值点.
![[Pasted image 20231019085952.png]]
从而我们得到了基本牛顿法的几个特点
**优点**
- 二阶收敛速度
- 二次终止性
**缺点**
- 依赖于初值选取
- 计算量过大
  - 每次计算 Hesse 矩阵,一共 $\frac{n(n+1)}{2}$个
  - 每一步迭代求解一个线性方程组,计算量是 $O(n^3)$.
## 阻尼牛顿方法
> [! 目的]
> 为了改进 Newton 方法的局部收敛性(但是也不能保证对于任意的初始点都收敛)

牛顿方法的方向都是由牛顿方程组确定的,下面我们开始考虑步长的选取,而使得性质更加良好.并且对于改进后的良好的性质,我们有下面的性质取保证.

>[!explanation for THM3.6]
>定理 3.6 核心作用在于告诉我们对于
>- 严格凸函数
>- 步长寻找满足 wolfe(或 Goldstein)准则
>- 方向是 Newton 方向
>的迭代是严格收敛的

![[OM-Thm3.6]]

下面的两种方法
>[!目的]
>为了解决 Newton 方法在迭代过程中出现的
>-  Hesse 矩阵奇异,不正定的情形
>- Newton 方向与 $g_k$几乎正交的情形
>解决方式:
>- 与其他方法混合[[Chapter3 负梯度方法与 Newton 型方法#混合方法]]
>- 对 Hesse 矩阵修正[[Chapter3 负梯度方法与 Newton 型方法#^57d3cd]]
## 混合方法

1. $G_k$可逆但不正定
由于 $G_k$不正定,那么 $g_KG^{-1}g_k^T<0$ 此时我们取与原来的方向相反的方向,也就是
$$
 d_k = G^{-1}g_k
$$

2. **Newton 方法与负梯度方法混合**
基本想法:当其中一种无法进行的时候,采用另一种方法.
```
if Gk 奇异&& gk与 dk 几乎正交:
	负梯度方向
elif Gk负定,Gk.inverse()存在:
	dk=Gk.inverse()gk
else
	to be continued
```

^60aab6

>[!Algorithm]
>混合方法


## LM 方法

>[!目的]
>在上面的混合方法中,对于其中的[[Chapter3 负梯度方法与 Newton 型方法#^60aab6|to be continued]]的部分,我们现在来进行解决,也就是***当 $G_k$奇异,不正定***的情形.

也就是考虑求解
$$
	(G_k+\nu_k I)d=-g_k
$$

^57d3cd
- $\nu_k$大小对于方向的影响
$\nu_k$很小的时候,方程组的解偏向于Newton 方法. 随着 $\nu_k$增大方程组的解向负梯度方向偏移.具体的解释将在后面的最小二乘方法章节中进行讨论.此处我们一般取 $\nu_k:=2\nu_k$.
那么,
$$
d=-(G_k+\nu_k I)
$$
我们对前面的 基本Newton方法作出改进
```python
import numpy as np
import matplotlib.pyplot as plt
from plot import plfunc
np.set_printoptions (suppress= True)

def armijo_wolfe(x, d, f, g, alpha=0.9, rho=1e-3, sigma=0.8):
    f0 = f(x)
    g0 = g(x)
    m = np.dot(g0, d)
    alpha_max = 1e10

    while alpha > 1e-10:
        x_new = x + alpha * d
        f_new = f(x_new)
        if f_new > f0 + rho * alpha * m or (f_new >= f(x) and alpha < 1e-10):
            #区间限制
            #armijo准则实现层,armijo 是为了避免 alpha 过大
            alpha_max = alpha
            alpha = 0.5 * (0 + alpha_max)
        else:
            g_new = g(x_new)
            if np.dot(g_new, d) < sigma * m or np.dot(g_new, d) > -sigma * m:
                return alpha
            elif np.dot(g_new, d) >= 0:
                alpha_max = alpha
                alpha = 0.5 * (0 + alpha_max)
            else:
                return alpha

    return alpha_max

def lm_method(x0, f, g, H, max_iter=100, tol=1e-3, mu=1e-4):
    x = np.array([x0])#可以适用于任何函数
    gk = []

    for k in range(max_iter):
        gg = g(x[-1])
        Hessian = H(x[-1])
        I = np.eye(len(x0))

        # Levenberg-Marquardt modification
        LM_matrix = Hessian + mu * I
        d = -np.linalg.inv(LM_matrix) @ gg.T
        alpha = armijo_wolfe(x[-1], d, f, g)
        xk = x[-1] + alpha * d
        x = np.r_[x, [xk]]
        gk.append(np.linalg.norm(gg))

        # Update mu
        mu *= 2. if np.linalg.norm(d) < tol else 0.5
        #用方向的大小去反应Hessian的情况,若方向很小,说明矩阵不可逆,那么就修正,否则不修正(2*0.5=1)

        # Check convergence
        if np.linalg.norm(gg) < tol:
            print("Converged successfully.")
            break

    else:
        print("Exceeded maximum iterations.")

    return x, gk

def func(x):
    return 3 * x[0]**2 + 3 * x[1]**2 - (x[0]**2) * x[1]

def g(x):
    return np.array([6 * x[0] - 2 * x[0] * x[1], 6 * x[1] - x[0]**2])

def H(x):
    return np.array([[6 - 2 * x[1], -2 * x[0]], [-2 * x[0], 6]])

x0 = np.array([1.5,1.5])
result, gk = lm_method(x0, func, g, H)

print(result)
print(len(result))
print(gk)
xx = result.T[0]
yy = result.T[1]

# plot
plfunc(func,result,[-6,6],[-6,6])



```

- 迭代 alpha 步长率为 0.5 可行,但是 0.618 不可以
--------
# Sect 3.3 拟Newton 方法
Ref:[最优化方法复习笔记（四）拟牛顿法](https://zhuanlan.zhihu.com/p/306635632).
## Summary of the sketch
**拟牛顿法是 $||\cdot||_{B_k}$下的最速下降法**
在Newton法的求解过程中,我们使用了二阶的 Taylor 展开,并在领域内舍弃了二次的高阶无穷小量,得到了牛顿方程

>[!思想]
>用一阶导数信息去近似 Hesse 矩阵 $B_{k+1}$

更进一步,我们直接构造矩阵 $H_{k+1}$ 去近似 $G_k^{-1}$.那么得到了下降方向
$$
 d=- H_{k+1}g_{k+1}
$$
我们要求近似矩阵满足
>[!条件]
>- 只需 $f(x)$ 的一阶导数信息
>- $B_{k+1}(H_{k+1})$正定,从而保证方向下降
>- 方法具有较快的收敛速度

要求得上述矩阵 $H_{k+1}(B_{k+1})$,我们先研究所需要满足的具体的条件形式
## 拟牛顿条件
我们首先研究较为简单的矩阵 $B_{k+1}$.利用一阶导数的泰勒展开,我们知道
$$
G_{k+1}(x_k-x_{k+1})=g_{k+1}-g_k+O(||s_k||^2)
$$
我们丢弃 $O(||x_k-x_{k+1}||^2)$之后得到了 $B_{k+1}$ 满足的条件
$$
B_{k+1}(x_{k+1}-x_k)=g_{k+1}-g_k
$$
那么 $H_{k+1}$满足
> [!math|{"type":"theorem","number":"auto","setAsNoteMathLink":false,"title":"Quasi Newton ","label":"quasi-newton-","_index":2}] Theorem 3 (Quasi Newton ).
> $$
>x_{k+1}-x_k=H_{k+1}(g_{k+1}-g_k)
>$$

从而得到了下面的方法

> [!math|{"type":"theorem","number":"auto","setAsNoteMathLink":false,"label":"","_index":3,"title":"拟牛顿方法"}] Theorem 4 (拟牛顿方法).
> 1. 给定 $x_0 \in \mathbb{R}^n$ ,对称正定矩阵 $H_0\in \mathbb{R}^{n\times n},\epsilon_0>0,k:=0$
> 2. 终止准则
> 3. 计算 $d_k=-H_kg_k$
> 4. 沿着 $d_k$ 方向线搜索 $\alpha_k$, $x_{k+1}=x_k+\alpha_k d_k$
> 5. 修正 $H_k$ 得到 $H_{k+1}$,使得 $H_{k+1}$ 满足上面的条件, $k:=k+1$,转 2

> [!math|{"type":"remark","number":"auto","setAsNoteMathLink":false,"_index":4}] Remark 5.
> - $H_0$通常取单位矩阵,此时第一步方向为负梯度方向

下面我们对拟合矩阵进行迭代(修正),我们一般以
$$
H_{k+1}=H_k+\Delta H_k
$$
的方式进行修正(迭代),下面我们对 $\Delta H_k$的取法进行研究.直观的,他应该具有简单,计算量小,有效的特点.下面截取了几种方式,但实际上方式还有很多.
## 拟Newton 方法的迭代
### 对称秩1 公式
Symmetric Rank1 SR1 公式.我们取 $\Delta H_k$为对称秩1 矩阵,那么就有
$$
 H_{k+1}=H_k+\beta uu^T, u \in \mathbb{R}^n,\beta \in \mathbb{R}
$$

[当 $s_k-H_ky_k=0$时候的算法](https://zhuanlan.zhihu.com/p/144736223)

## 拟牛顿法的算法实现
#算法实现
https://zhuanlan.zhihu.com/p/92773676
### SR1 方法
```python
import numpy as np
import matplotlib.pyplot as plt
from plot import plfunc
np.set_printoptions(suppress=True)
def armijo_wolfe(x, d, f, g, alpha=0.9, rho=1e-3, sigma=0.8):
    f0 = f(x)
    g0 = g(x)
    m = np.dot(g0, d)
    alpha_max = 1e10

    while alpha > 1e-10:
        x_new = x + alpha * d
        f_new = f(x_new)
        if f_new > f0 + rho * alpha * m or (f_new >= f(x) and alpha < 1e-10):
            # 区间限制
            # armijo准则实现层,armijo 是为了避免 alpha 过大
            alpha_max = alpha
            alpha = 0.618 * (0 + alpha_max)
        else:
            g_new = g(x_new)
            print(m)
            if np.dot(g_new, d) < sigma * m or np.dot(g_new, d) > -sigma * m:
                alpha_max = alpha
                alpha = 0.618 * (0 + alpha_max)
            elif np.dot(g_new, d) >= 0:
                alpha_max = alpha
                alpha = 0.618 * (0 + alpha_max)
            else:
                return alpha

    return alpha_max

def sr1_update(B, s, y):
    y = y - B @ s
    B += np.outer(y, y) / np.dot(y, s)
    return B

def sr1_method(x0, f, g, max_iter=100, tol=1e-3, mu=1e-4):
    x = np.array([x0])
    gk = []
    Hessian = np.eye(len(x0))  # 初始估计的 Hessian 矩阵为单位矩阵

    for k in range(max_iter):
        gg = g(x[-1])
        d = -np.linalg.inv(Hessian) @ gg.T
        alpha = armijo_wolfe(x[-1], d, f, g)
        s = alpha * d
        xk = x[-1] + s
        x = np.r_[x, [xk]]
        gk.append(np.linalg.norm(gg))

        y = g(xk) - gg
        Hessian = sr1_update(Hessian, s, y)

        # Check convergence
        if np.linalg.norm(gg) < tol:
            print("Converged successfully.")
            break

    else:
        print("Exceeded maximum iterations.")

    return x, gk

def func(x):
    return 3 * x[0]**2 + 3 * x[1]**2 - (x[0]**2) * x[1]

def g(x):
    return np.array([6 * x[0] - 2 * x[0] * x[1], 6 * x[1] - x[0]**2])

x0 = np.array([-2,4])
result, gk = sr1_method(x0, func, g)

print(result)
print(len(result))
print(gk)
xx = result.T[0]
yy = result.T[1]

# plot
plfunc(func, result, [-6, 6], [-6, 6])

```
### DFP
```python
import numpy as np
import matplotlib.pyplot as plt
from plot import plfunc
np.set_printoptions(suppress=True)

def armijo_wolfe(x, d, f, g, alpha=0.9, rho=1e-3, sigma=0.8):
    f0 = f(x)
    g0 = g(x)
    m = np.dot(g0, d)
    alpha_max = 1e10

    while alpha > 1e-10:
        x_new = x + alpha * d
        f_new = f(x_new)
        if f_new > f0 + rho * alpha * m or (f_new >= f(x) and alpha < 1e-10):
            # 区间限制
            # armijo准则实现层,armijo 是为了避免 alpha 过大
            alpha_max = alpha
            alpha = 0.618 * (0 + alpha_max)
        else:
            g_new = g(x_new)
            if np.dot(g_new, d) < sigma * m or np.dot(g_new, d) > -sigma * m:
                alpha_max = alpha
                alpha = 0.618 * (0 + alpha_max)
            elif np.dot(g_new, d) >= 0:
                alpha_max = alpha
                alpha = 0.618 * (0 + alpha_max)
            else:
                return alpha

    return alpha_max

def dfp_update(H, s, y):
    Hy = H @ y
    return H + np.outer(s, s) / np.dot(s, y) - np.outer(Hy, Hy) / np.dot(y, Hy)

def dfp_method(x0, f, g, max_iter=100, tol=1e-3):
    x = np.array([x0])
    gk = []
    H = np.eye(len(x0))  # 初始估计的 Hessian 逆矩阵

    for k in range(max_iter):
        gg = g(x[-1])
        d = -H @ gg.T
        alpha = armijo_wolfe(x[-1], d, f, g)
        s = alpha * d
        xk = x[-1] + s
        x = np.r_[x, [xk]]
        gk.append(np.linalg.norm(gg))

        y = g(xk) - gg
        H = dfp_update(H, s, y)

        # Check convergence
        if np.linalg.norm(gg) < tol:
            print("Converged successfully.")
            break

    else:
        print("Exceeded maximum iterations.")

    return x, gk

def func(x):
    return 3 * x[0]**2 + 3 * x[1]**2 - (x[0]**2) * x[1]

def g(x):
    return np.array([6 * x[0] - 2 * x[0] * x[1], 6 * x[1] - x[0]**2])

x0 = np.array([0,3])
result, gk = dfp_method(x0, func, g)

print(result)
print(len(result))
print(gk)
xx = result.T[0]
yy = result.T[1]

# plot
plfunc(func, result, [-6, 6], [-6, 6])

```
### BFGS
```python
import numpy as np
import matplotlib.pyplot as plt
from plot import plfunc
np.set_printoptions(suppress=True)

def armijo_wolfe(x, d, f, g, alpha=0.9, rho=1e-3, sigma=0.8):
    f0 = f(x)
    g0 = g(x)
    m = np.dot(g0, d)
    alpha_max = 1e10

    while alpha > 1e-10:
        x_new = x + alpha * d
        f_new = f(x_new)
        if f_new > f0 + rho * alpha * m or (f_new >= f(x) and alpha < 1e-10):
            # 区间限制
            # armijo准则实现层,armijo 是为了避免 alpha 过大
            alpha_max = alpha
            alpha = 0.618 * (0 + alpha_max)
        else:
            g_new = g(x_new)
            if np.dot(g_new, d) < sigma * m or np.dot(g_new, d) > -sigma * m:
                alpha_max = alpha
                alpha = 0.618 * (0 + alpha_max)
            elif np.dot(g_new, d) >= 0:
                alpha_max = alpha
                alpha = 0.618 * (0 + alpha_max)
            else:
                return alpha

    return alpha_max

def BFGS_update(B, s, y):
    Bs = B @ s
    return B + np.outer(y, y) / np.dot(y, s) - np.outer(Bs, Bs) / np.dot(s, Bs)

def dfp_method(x0, f, g, max_iter=100, tol=1e-3):
    x = np.array([x0])
    gk = []
    B = np.eye(len(x0))  # 初始估计的 Hessian 逆矩阵

    for k in range(max_iter):
        gg = g(x[-1])
        d = -np.linalg.inv(B) @ gg.T
        alpha = armijo_wolfe(x[-1], d, f, g)
        s = alpha * d
        xk = x[-1] + s
        x = np.r_[x, [xk]]
        gk.append(np.linalg.norm(gg))

        y = g(xk) - gg
        B = BFGS_update(B, s, y)

        # Check convergence
        if np.linalg.norm(gg) < tol:
            print("Converged successfully.")
            break

    else:
        print("Exceeded maximum iterations.")

    return x, gk

def func(x):
    return 3 * x[0]**2 + 3 * x[1]**2 - (x[0]**2) * x[1]

def g(x):
    return np.array([6 * x[0] - 2 * x[0] * x[1], 6 * x[1] - x[0]**2])

x0 = np.array([-2,4])
result, gk = dfp_method(x0, func, g)

print(result)
print(len(result))
print(gk)
xx = result.T[0]
yy = result.T[1]

# plot
plfunc(func, result, [-6, 6], [-6, 6])
```


